{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LavanyaPobbathi/Metrics-for-Text-Summarization/blob/main/GPT4_Research_Paper_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install pdfplumber\n",
        "!pip install requests\n",
        "!pip install arxiv"
      ],
      "metadata": {
        "id": "aWmOnnJcByUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf06ba5-743d-4602-b0e9-e74325fe2868"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/77.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.10.3-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.0/49.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20221105 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.23.1-py3-none-manylinux_2_17_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (3.3.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20221105->pdfplumber) (41.0.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
            "Installing collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20221105 pdfplumber-0.10.3 pypdfium2-4.23.1\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Collecting arxiv\n",
            "  Downloading arxiv-2.0.0-py3-none-any.whl (11 kB)\n",
            "Collecting feedparser==6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.10-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.31.0)\n",
            "Collecting sgmllib3k (from feedparser==6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0->arxiv) (2023.7.22)\n",
            "Building wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=f3ab40f3d43beb4b700075348b7cf0ad7f9bead3eb103c8f7a52e40576dde869\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.0.0 feedparser-6.0.10 sgmllib3k-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BrVum-Yr8_Jq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import openai\n",
        "import json\n",
        "import time\n",
        "from openai.error import RateLimitError\n",
        "import tiktoken\n",
        "import requests\n",
        "import pdfplumber\n",
        "import io\n",
        "from IPython.display import Markdown\n",
        "import arxiv\n",
        "\n",
        "\n",
        "# TODO - Copy your Open AI Secret Key here\n",
        "openai.api_key = \"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getPDFText(url):\n",
        "  # download the file\n",
        "  response = requests.get(url)\n",
        "\n",
        "  document_text = \"\"\n",
        "\n",
        "  # load the PDF file using pdfplumber\n",
        "  with pdfplumber.open(io.BytesIO(response.content)) as pdf:\n",
        "      # iterate over all the pages in the PDF file\n",
        "      for page in pdf.pages:\n",
        "          # extract the text from the page\n",
        "          text = page.extract_text()\n",
        "\n",
        "          document_text = document_text + \" \" + text\n",
        "\n",
        "  return document_text\n",
        "\n",
        "def getAbstract(paper_id):\n",
        "  # Search for the paper with the given ID\n",
        "  search = arxiv.Search(id_list=[paper_id])\n",
        "  paper = next(search.results())\n",
        "\n",
        "  # Extract the abstract text\n",
        "  abstract_text = paper.summary\n",
        "  return abstract_text"
      ],
      "metadata": {
        "id": "AAriihuC5NU7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize(document_prompt, user_prompt, system_prompt, model, temp=.7, tokens=750):\n",
        "  response = openai.ChatCompletion.create(\n",
        "          model=model,\n",
        "          messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "                    {\"role\": \"user\", \"content\": document_prompt},\n",
        "                    {\"role\": \"user\", \"content\": user_prompt}],\n",
        "          temperature=temp,\n",
        "          max_tokens=tokens,\n",
        "          top_p=1,\n",
        "          frequency_penalty=0,\n",
        "          presence_penalty=0\n",
        "        )\n",
        "  output_summary = response[\"choices\"][0][\"message\"][\"content\"]\n",
        "  return output_summary\n"
      ],
      "metadata": {
        "id": "Bih_6g4hGWd3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Paper #1"
      ],
      "metadata": {
        "id": "EcwqT_wTsSJh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_text = getPDFText(\"https://arxiv.org/pdf/2304.00228.pdf\")\n",
        "abstract_text = getAbstract(\"2304.00228\")"
      ],
      "metadata": {
        "id": "4J7W1kv55Hr6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3373f614-a823-4760-ca3c-46629917045c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-42af0539cf4f>:21: DeprecationWarning: The '(Search).results' method is deprecated, use 'Client.results' instead\n",
            "  paper = next(search.results())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a research assistant. Return your results in Markdown format.\"\n",
        "document_prompt = \"You will accurately answer questions about the following research paper: \" + document_text\n",
        "model = \"gpt-4\"\n",
        "extracted_info = []"
      ],
      "metadata": {
        "id": "lcf9sjlz5-Cz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"What research question is the paper trying to answer? Explain what the researchers did to\\\n",
        " study that question, including the specific methods used and analyses performed. Explain thoroughly and be sure to include specific details.\"\n",
        "t = summarize(document_prompt, user_prompt, system_prompt, model)\n",
        "display(Markdown(t))\n",
        "extracted_info.append(t)"
      ],
      "metadata": {
        "id": "Ge4EQUEzSYsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "d2d0c20a-eed8-4d0a-c600-3c80ec7fae4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The research paper is investigating whether Large Language Models (LLMs), specifically ChatGPT, can evaluate the credibility of various news outlets. The paper's central question is, \"Can ChatGPT rate the credibility of news outlets?\"\n\nTo study this question, the researchers used a series of methods and analyses:\n\n1. **Prompting ChatGPT:** They prompted ChatGPT to provide credibility ratings for over 7,000 news domains. The prompt instructed the model to rate the website's credibility on a scale from 0 to 1, where 0 means very low credibility and 1 means very high credibility. ChatGPT was to return a rating of -1 if it had no knowledge of the website. Otherwise, it was to provide the best estimation of the site's credibility.\n\n2. **Analyzing ChatGPT's Responses:** They analyzed the answers returned by ChatGPT to find if it could follow the instructions and yield ratings in the range between 0 and 1 with one decimal point of precision. \n\n3. **Correlation with Human Expert Judgments:** The researchers compared the ratings given by ChatGPT with those from human experts to measure their alignment. They used the aggregate human ratings produced by Lin et al., which were derived from multiple sources. Additionally, they took into account the ratings assigned by Media Bias/Fact Check (MBFC) and NewsGuard.\n\n4. **Classifying Domains:** The researchers classified domains as either low- or high-credibility and compared the ChatGPT ratings with these ground-truth labels. They used the AUC (Area Under the Receiver Operating Characteristic Curve) score to quantify the performance of ChatGPT.\n\n5. **Non-English and Satirical Domains:** The researchers also tested the performance of ChatGPT on non-English sources and satirical websites. They used the MBFC list of satire websites for this part of the study.\n\n6. **Data Availability and Processing:** All of the code and data used in the study are shared on GitHub. The researchers also provided information about how they queried the ChatGPT API, how they dealt with the human expert ratings, and how they measured the popularity of websites."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"What are the key findings reported in the paper that are important for journalists such as reporters and editors?\\\n",
        "How might journalists such as reporters and editors benefit from these findings? Why might there still be limits to those benefits? \\\n",
        " Explain thoroughly and be sure to include specific details.\"\n",
        "t = summarize(document_prompt, user_prompt, system_prompt, model)\n",
        "display(Markdown(t))\n",
        "extracted_info.append(t)"
      ],
      "metadata": {
        "id": "svej1MtaBiZe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "outputId": "9dd28d77-4801-429d-eedc-61d49c3e4ced"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper titled \"Large language models can rate news outlet credibility\" by Kai-Cheng Yang and Filippo Menczer from Indiana University Bloomington explores the ability of large language models (LLMs) such as ChatGPT to evaluate the credibility of news outlets. \n\nKey Findings:\n1. Using appropriate instructions, ChatGPT can provide ratings for a wide array of news outlets, including those in non-English languages and satirical sources. \n2. The ratings provided by ChatGPT significantly correlate with those from human experts (Spearmam’s ρ = 0.54,p < 0.001), suggesting that LLMs could be an affordable reference for credibility ratings in fact-checking applications.\n3. ChatGPT's performance on non-English news sources is consistent with its performance on English sources.\n4. ChatGPT can identify satirical websites with a 77.4% success rate. \n\nBenefits for Journalists:\n1. The ability of LLMs like ChatGPT to rate news outlets can be used as an additional tool for journalists to verify the credibility of a source quickly and cost-effectively.\n2. Since ChatGPT can evaluate non-English language sources, it can help journalists working with international sources to assess their credibility. \n3. The ability to identify satirical websites can prevent journalists from unintentionally referencing or utilizing misinformation from these sites.\n\nLimits to the Benefits:\n1. The accuracy of LLMs is not perfect. The study showed a moderate correlation between ChatGPT's ratings and expert ratings, meaning there can still be discrepancies.\n2. The study found that ChatGPT cannot always correctly identify satirical websites. This could lead to potential misinterpretations if journalists rely solely on ChatGPT for credibility assessment.\n3. The study used a specific prompt to instruct ChatGPT to rate the credibility of a news outlet. The results might differ with different instructions.\n4. The study focused solely on ChatGPT, and its findings may not generalize to all large language models. Different models might produce different results.\n5. The ratings provided by ChatGPT are based on its training data, which means it might not be aware of recent changes in the credibility of a news outlet. \n   \nOverall, while LLMs like ChatGPT can be a useful tool for journalists, they should not replace traditional fact-checking and source evaluation methods."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Critique the findings of the paper, focusing on their validity and utility for journalists such as reporters and editors.\\\n",
        "Are there reasons not to trust any of the findings? Explain thoroughly and be sure to include specific details.\"\n",
        "t = summarize(document_prompt, user_prompt, system_prompt, model)\n",
        "display(Markdown(t))\n",
        "extracted_info.append(t)"
      ],
      "metadata": {
        "id": "2OBZxNl2E7Hi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 475
        },
        "outputId": "7371d1b6-5b8a-4c54-b9ab-e15fb093e813"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The paper \"Large language models can rate news outlet credibility\" demonstrates that large language models (LLMs) like ChatGPT can evaluate the credibility of news outlets. The researchers found that these models' ratings moderately correlate with those of human experts and can rate a variety of news sources, including those in non-English languages and satirical sources.\n\nWhile these findings are promising, their validity and utility are not without limitations. For journalists, reporters, and editors, these are key points to consider:\n\n1. **Limited Robustness:** The study demonstrated that LLMs, specifically ChatGPT, can rate news outlet credibility. However, it's important to note that the results might not be generalizable across all LLMs. Different models may have different abilities to assess credibility based on their programming and training data.\n\n2. **Bias in Training Data:** The LLMs' ratings are based on the information embedded within their architecture. If the training data of these models contain biased or inaccurate information, the credibility ratings produced by the LLMs could be skewed. The paper does not detail how these potential biases were controlled for in their study.\n\n3. **Inconsistent Ratings:** While the paper shows that LLMs can rate news outlet credibility, it also acknowledges that the flexibility in the prompt provided to ChatGPT may lead to different outcomes. This inconsistency could limit the utility of such models in a journalistic context, where consistent and reliable information is crucial.\n\n4. **Language Limitations:** The paper acknowledges lower performance in rating Italian-language sources. This suggests that the language capabilities of the models may still need improvement, limiting their utility in assessing non-English sources.\n\n5. **Misidentification of Satirical Sources:** While ChatGPT was able to identify the satirical nature of 77.4% of the satirical sources, it failed to correctly identify the rest. This could lead to potential misinformation if these sources are incorrectly classified as credible.\n\n6. **Lack of Real-time Updating:** The LLMs are typically fixed after the training phase and thus may lack real-time updating of their knowledge, which can affect their assessment of news outlet credibility.\n\nIn conclusion, while the research presents an interesting exploration of LLMs in rating news outlet credibility, these limitations suggest that caution should be taken when considering their use in journalistic contexts. Further studies are needed to address these issues and enhance the models' performance in real-world applications."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a news reporter writing an article for an online blog. Return your results in Markdown format.\"\n",
        "document_prompt = \"Here is a research paper abstract: \" + abstract_text\n",
        "user_prompt = \"Here are some important observations about that research paper: \" + extracted_info[0] + \" \" + extracted_info[1] + \" \" + extracted_info[2] + \"\\n\\n \"\\\n",
        "+  \"Write a 600 word article about the paper using only the paper text and the important observations about \\\n",
        "the paper above and focusing on the benefits and limitations of the findings for journalists. \\\n",
        "Reduce scientific jargon and technical terminology in the writing.\"\n",
        "t = summarize(document_prompt, user_prompt, system_prompt, model, temp=0.1, tokens = 1200)\n",
        "display(Markdown(t))"
      ],
      "metadata": {
        "id": "C0jknqmQbX1T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "outputId": "e0fdab64-f480-4de7-fac8-a33190a27b02"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Large Language Models Can Rate News Outlet Credibility: A Boon or a Bane for Journalists?\n\nIn the era of information overload, the credibility of news outlets is a pressing concern. A recent study by researchers at Indiana University Bloomington has explored the potential of artificial intelligence, specifically Large Language Models (LLMs) like ChatGPT, in assessing the credibility of news outlets. While the findings are promising, they also highlight some limitations that journalists should consider.\n\n## The Potential of AI in Journalism\n\nThe study found that when given appropriate instructions, ChatGPT can provide credibility ratings for a wide array of news outlets, including those in non-English languages and satirical sources. The ratings provided by ChatGPT showed a significant correlation with those from human experts, suggesting that AI could be a cost-effective tool for credibility ratings in fact-checking applications.\n\nFor journalists, this could be a game-changer. The ability to quickly and cost-effectively verify the credibility of a source could streamline the fact-checking process. Furthermore, the ability to evaluate non-English language sources could be a boon for journalists working with international sources. \n\nThe study also found that ChatGPT can identify satirical websites with a 77.4% success rate. This could prevent journalists from unintentionally referencing or utilizing misinformation from these sites.\n\n## The Caveats\n\nHowever, the study also highlighted several limitations that journalists should be aware of. Firstly, the accuracy of AI is not perfect. The study showed a moderate correlation between ChatGPT's ratings and expert ratings, meaning there can still be discrepancies. \n\nSecondly, the study found that ChatGPT cannot always correctly identify satirical websites. This could lead to potential misinterpretations if journalists rely solely on AI for credibility assessment.\n\nThirdly, the study used a specific prompt to instruct ChatGPT to rate the credibility of a news outlet. The results might differ with different instructions. This inconsistency could limit the utility of such models in a journalistic context, where consistent and reliable information is crucial.\n\nFourthly, the study focused solely on ChatGPT, and its findings may not generalize to all large language models. Different models might produce different results.\n\nLastly, the ratings provided by AI are based on its training data, which means it might not be aware of recent changes in the credibility of a news outlet. \n\n## The Bottom Line\n\nWhile AI, like ChatGPT, can be a useful tool for journalists, they should not replace traditional fact-checking and source evaluation methods. The study's findings suggest that AI can provide a helpful starting point in assessing the credibility of a news outlet, but it should be used in conjunction with other fact-checking methods.\n\nIn conclusion, the use of AI in journalism presents both opportunities and challenges. As AI continues to evolve, it's crucial for journalists to stay informed about its potential benefits and limitations. The key is to strike a balance between leveraging AI's capabilities and maintaining rigorous journalistic standards."
          },
          "metadata": {}
        }
      ]
    }
  ]
}